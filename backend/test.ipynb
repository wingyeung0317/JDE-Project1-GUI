{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request\n",
    "# from flask_sqlalchemy import SQLAlchemy\n",
    "from flask_cors import CORS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import re\n",
    "import demoji\n",
    "import datetime\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from wordcloud import STOPWORDS\n",
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from keybert import KeyBERT\n",
    "from importlib import reload\n",
    "import matplotlib.dates as mdates\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.engine import URL\n",
    "import snscrape.modules.facebook as snfacebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c1149593b6847f98a376ab9cac4b834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading .gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3058aff8a69d4a25a122fbc461e5133b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading 1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71f99c54a9e44803b829eee1e36af40d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c45e507325014a80a9be7363960940f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6db40ab128147b8bc6f6148c9dbf98f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ce_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71a2038b74e2460f96edd3708a1e28b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2fefa0d281b41caad1992562c50698d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efc9a8466127410bab81c520d3cd1965",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b94e6fd1fdc243e6b1dc2783240b173b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "621a8970445c4805bdb5bb3f79e10e16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd7560f050fa41089d0275f5f63a83a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ba0cbbfcc3a4f06b7c36b645a1a4ca8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading train_script.py:   0%|          | 0.00/13.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c586726b1a5489aa9e91d54f3be501c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4c37e3709a743178084f78ec67e68b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt=reload(plt)\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:101.0) Gecko/20100101 Firefox/101.0'}\n",
    "stopwords = STOPWORDS.update(['https', 't co', 't', 'co'])\n",
    "like_pngfile = './like.png'\n",
    "dislike_pngfile = './dislike.png'\n",
    "loading_pngfile = './loading.jpeg'\n",
    "like_color_mask = np.array(Image.open(like_pngfile))\n",
    "dislike_color_mask = np.array(Image.open(dislike_pngfile))\n",
    "loading_color_mask = np.array(Image.open(loading_pngfile))\n",
    "kw_model = KeyBERT(model='all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_in_tweets(df):\n",
    "    linkInTweets = pd.DataFrame()\n",
    "    for i in range(0, len(df)):\n",
    "        for url in findURL(df.loc[i,'content']):\n",
    "            try:\n",
    "                response = requests.get(url, headers=headers, allow_redirects=True)\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "                soupText = rmEmoji(rmURL(soup.getText()))\n",
    "                linkInTweets = pd.concat([pd.DataFrame({'text_in_url': [soupText], 'sa':[sentiment_scores(soupText)['compound']], 'url':[url], 'tweet_id':[df.loc[i, 'id']]}), linkInTweets], ignore_index=True)\n",
    "                linkInTweets['text_in_url'] = linkInTweets['text_in_url'].str.replace(\"\\n\", \" \")\n",
    "            except:\n",
    "                print(f'{url} not accessable')\n",
    "        # if i == 7:\n",
    "        #     break\n",
    "    return linkInTweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reFilterSpace(text):\n",
    "    text = str(text)\n",
    "    # if text != '':\n",
    "    #     while text[0] == ' ':\n",
    "    #         text = text[1:]\n",
    "    #     while text[-1] == ' ':\n",
    "    #         text = text[:-1]\n",
    "    text=text.strip()\n",
    "    return text\n",
    "\n",
    "def rmURL(content, condition=True):\n",
    "    return (re.sub(r'http\\S+', '', content) if condition else content)\n",
    "\n",
    "def findURL(content):\n",
    "    return re.findall(r'http\\S+', content)\n",
    "\n",
    "def rmEmoji(content):\n",
    "    return demoji.replace(content, repl='')\n",
    "\n",
    "def remove_newlines(text):\n",
    "    return re.sub(r'\\n', ' ', text)\n",
    "\n",
    "def remove_hashtags(text):\n",
    "    return re.sub(r'\\#\\w+', ' ', text)\n",
    "\n",
    "def remove_actags(text):\n",
    "    return re.sub(r'\\@\\w+', ' ', text)\n",
    "\n",
    "def remove_cashtags(text):\n",
    "    return re.sub(r'\\$[a-zA-Z]+', ' ', text) \n",
    "\n",
    "def remove_num(text):\n",
    "    return re.sub(r'\\d+', ' ', text) \n",
    "\n",
    "def remove_punc(text):\n",
    "    return re.sub(    r'[~_%$+()=-]+'    , ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_scores(sentence):\n",
    "    sid_obj = SentimentIntensityAnalyzer()\n",
    "    sentiment_dict = sid_obj.polarity_scores(sentence)\n",
    "    return sentiment_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" def grapTweets(id, name, cashTag, qFilter, qFilterLinks, qFilterReplies, lang, qFilterVerified, qLocation, qStartTime, qEndTime, qWithinTime, qMinLike, qMinRetweets, qMinReplies, sa_rmEmoji, sa_rmNewLine, sa_rmHashtag, sa_rmCashtag, sa_rmACtag, sa_rmPunc, sa_rmNum, samples):\n",
    "    # init #\n",
    "    orCashTag = ''\n",
    "    qFilterText = ''\n",
    "    addfilter = ''\n",
    "    cashTag = str(cashTag)\n",
    "    qFilter = str(qFilter)\n",
    "    qWithinTime = reFilterSpace(str(qWithinTime))\n",
    "\n",
    "    ### Transform User input to Query ###\n",
    "    if (cashTag != '') or (cashTag=='nan'):\n",
    "        for text in cashTag.split(','):\n",
    "            orCashTag += f' OR {reFilterSpace(text)}'\n",
    "    if (qFilter != '') or (qFilter=='nan'):\n",
    "        for text in qFilter.split(','):\n",
    "            qFilterText += f' -\"{reFilterSpace(text)}\"'\n",
    "    if qFilterLinks:\n",
    "        addfilter += ' -filter:links'\n",
    "    if qFilterReplies:\n",
    "        addfilter += ' -filter:replies'\n",
    "    if qFilterVerified:\n",
    "        addfilter += ' filter:verified'\n",
    "    if type(qLocation)==str: # NaN's type is float\n",
    "        addfilter += f' near:\"{reFilterSpace(qLocation)}\"' if qLocation!='' else ''\n",
    "    if (qWithinTime == '') or (qWithinTime=='nan'):\n",
    "        if str(qStartTime) != 'nan':\n",
    "            addfilter += f' since:{qStartTime}'\n",
    "        if str(qEndTime) != 'nan':\n",
    "            addfilter += f' until:{qEndTime}'\n",
    "    else:\n",
    "        addfilter += f' within_time:{qWithinTime}'\n",
    "\n",
    "    ### Grapping ###\n",
    "    queryText = f'{name}{orCashTag}{qFilterText}{addfilter} lang:{lang} min_retweets:{qMinRetweets} min_faves:{qMinLike} min_replies:{qMinReplies}'\n",
    "    scraper = sntwitter.TwitterSearchScraper(queryText)\n",
    "    print(queryText)\n",
    "    tweets = []\n",
    "    for i, tweet in enumerate(scraper.get_items()):\n",
    "        sa_content = rmEmoji(remove_newlines(rmURL(tweet.content))) if sa_rmEmoji else remove_newlines(rmURL(tweet.content))\n",
    "        sa_content = remove_newlines(sa_content) if sa_rmNewLine else sa_content\n",
    "        sa_content = remove_hashtags(sa_content) if sa_rmHashtag else sa_content\n",
    "        sa_content = remove_cashtags(sa_content) if sa_rmCashtag else sa_content\n",
    "        sa_content = remove_actags(sa_content) if sa_rmACtag else sa_content\n",
    "        sa_content = remove_punc(sa_content) if sa_rmPunc else sa_content\n",
    "        sa_content = remove_num(sa_content) if sa_rmNum else sa_content\n",
    "        sa_score = (TextBlob(sa_content).polarity + sentiment_scores(sa_content)['compound'])/2\n",
    "        data = [\n",
    "            id,\n",
    "            tweet.date,\n",
    "            str(tweet.id),\n",
    "            tweet.content,\n",
    "            tweet.user.username,\n",
    "            tweet.likeCount,\n",
    "            tweet.retweetCount,\n",
    "            tweet.url,\n",
    "            name,\n",
    "            sa_score,\n",
    "            sa_content,\n",
    "        ]\n",
    "        tweets.append(data)\n",
    "        ### Grap how many Data ###\n",
    "        if i==samples:\n",
    "            break\n",
    "    returnDF = pd.DataFrame(tweets, columns=['index', 'date', 'id', 'content', 'username', 'likes', 'retweets', 'url', 'from_query_name', 'sa_score', 'sa_content'])\n",
    "    returnDF['content'] = returnDF['content'].str.replace(\"\\n\", \" \")\n",
    "    # returnDF['date'] = pd.to_datetime(returnDF['date'])\n",
    "    return returnDF \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grapfbposts(id, name, cashTag, qFilter, qFilterLinks, qFilterReplies, lang, qFilterVerified, qLocation, qStartTime, qEndTime, qWithinTime, qMinLike, qMinRetweets, qMinReplies, sa_rmEmoji, sa_rmNewLine, sa_rmHashtag, sa_rmCashtag, sa_rmACtag, sa_rmPunc, sa_rmNum, samples):\n",
    "    # init #\n",
    "    orCashTag = ''\n",
    "    qFilterText = ''\n",
    "    addfilter = ''\n",
    "    cashTag = str(cashTag)\n",
    "    qFilter = str(qFilter)\n",
    "    qWithinTime = reFilterSpace(str(qWithinTime))\n",
    "\n",
    "    ### Transform User input to Query ###\n",
    "    if (cashTag != '') or (cashTag=='nan'):\n",
    "        for text in cashTag.split(','):\n",
    "            orCashTag += f' OR {reFilterSpace(text)}'\n",
    "    if (qFilter != '') or (qFilter=='nan'):\n",
    "        for text in qFilter.split(','):\n",
    "            qFilterText += f' -\"{reFilterSpace(text)}\"'\n",
    "    if qFilterLinks:\n",
    "        addfilter += ' -filter:links'\n",
    "    if qFilterReplies:\n",
    "        addfilter += ' -filter:replies'\n",
    "    if qFilterVerified:\n",
    "        addfilter += ' filter:verified'\n",
    "    if type(qLocation)==str: # NaN's type is float\n",
    "        addfilter += f' near:\"{reFilterSpace(qLocation)}\"' if qLocation!='' else ''\n",
    "    if (qWithinTime == '') or (qWithinTime=='nan'):\n",
    "        if str(qStartTime) != 'nan':\n",
    "            addfilter += f' since:{qStartTime}'\n",
    "        if str(qEndTime) != 'nan':\n",
    "            addfilter += f' until:{qEndTime}'\n",
    "    else:\n",
    "        addfilter += f' within_time:{qWithinTime}'\n",
    "\n",
    "    ### Grapping ###\n",
    "    queryText = f'{name}{orCashTag}{qFilterText}{addfilter} lang:{lang} min_retweets:{qMinRetweets} min_faves:{qMinLike} min_replies:{qMinReplies}'\n",
    "    scraper = sntwitter.TwitterSearchScraper(queryText)\n",
    "    print(queryText)\n",
    "    tweets = []\n",
    "    for i, tweet in enumerate(scraper.get_items()):\n",
    "        sa_content = rmEmoji(remove_newlines(rmURL(tweet.content))) if sa_rmEmoji else remove_newlines(rmURL(tweet.content))\n",
    "        sa_content = remove_newlines(sa_content) if sa_rmNewLine else sa_content\n",
    "        sa_content = remove_hashtags(sa_content) if sa_rmHashtag else sa_content\n",
    "        sa_content = remove_cashtags(sa_content) if sa_rmCashtag else sa_content\n",
    "        sa_content = remove_actags(sa_content) if sa_rmACtag else sa_content\n",
    "        sa_content = remove_punc(sa_content) if sa_rmPunc else sa_content\n",
    "        sa_content = remove_num(sa_content) if sa_rmNum else sa_content\n",
    "        sa_score = (TextBlob(sa_content).polarity + sentiment_scores(sa_content)['compound'])/2\n",
    "        data = [\n",
    "            id,\n",
    "            tweet.date,\n",
    "            str(tweet.id),\n",
    "            tweet.content,\n",
    "            tweet.user.username,\n",
    "            tweet.likeCount,\n",
    "            tweet.retweetCount,\n",
    "            tweet.url,\n",
    "            name,\n",
    "            sa_score,\n",
    "            sa_content,\n",
    "        ]\n",
    "        tweets.append(data)\n",
    "        ### Grap how many Data ###\n",
    "        if i==samples:\n",
    "            break\n",
    "    returnDF = pd.DataFrame(tweets, columns=['index', 'date', 'id', 'content', 'username', 'likes', 'retweets', 'url', 'from_query_name', 'sa_score', 'sa_content'])\n",
    "    returnDF['content'] = returnDF['content'].str.replace(\"\\n\", \" \")\n",
    "    # returnDF['date'] = pd.to_datetime(returnDF['date'])\n",
    "    return returnDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def returnGrap(row, input, samples=20):\n",
    "    # print(\n",
    "    return grapTweets(\n",
    "        input.loc[row, 'id'],\n",
    "        input.loc[row, 'name'],\n",
    "        input.loc[row, 'cashtag'],\n",
    "        input.loc[row, 'qFilter'],\n",
    "        input.loc[row, 'qFilterLinks'],\n",
    "        input.loc[row, 'qFilterReplies'],\n",
    "        input.loc[row, 'lang'],\n",
    "        input.loc[row, 'qFilterVerified'],\n",
    "        input.loc[row, 'qLocation'],\n",
    "        input.loc[row, 'qStartTime'],\n",
    "        input.loc[row, 'qEndTime'],\n",
    "        input.loc[row, 'qWithinTime'],\n",
    "        input.loc[row, 'qMinLike'],\n",
    "        input.loc[row, 'qMinRetweets'],\n",
    "        input.loc[row, 'qMinReplies'],\n",
    "        input.loc[row, 'sa_rmEmoji'],\n",
    "        input.loc[row, 'sa_rmNewLine'],\n",
    "        input.loc[row, 'sa_rmHashtag'],\n",
    "        input.loc[row, 'sa_rmCashtag'],\n",
    "        input.loc[row, 'sa_rmACtag'],\n",
    "        input.loc[row, 'sa_rmPunc'],\n",
    "        input.loc[row, 'sa_rmNum'],\n",
    "        samples\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphPlot(df):\n",
    "    global plt\n",
    "    plt=reload(plt)\n",
    "    plt.scatter(df['date'], df['sa_score'])\n",
    "    plt.axis(True)\n",
    "    plt.xlabel='Date'\n",
    "    plt.ylabel='Sentiment'\n",
    "    plt.axis(ymin=-1, ymax=1)\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%m/%d/%Y'))\n",
    "    plt.gca().xaxis.set_major_locator(mdates.DayLocator())\n",
    "    plt.gcf().autofmt_xdate()\n",
    "    __tmpfile = BytesIO()\n",
    "    plt.savefig(__tmpfile, format='png')\n",
    "    __encoded = base64.b64encode(__tmpfile.getvalue()).decode('utf-8')\n",
    "    imghtml = '<img class=\"pltimg\" src=\\'data:image/png;base64,{}\\'>'.format(__encoded)\n",
    "    plt.close()\n",
    "    return imghtml\n",
    "\n",
    "def graphKeyword(df, sentiment):\n",
    "    global plt\n",
    "    plt=reload(plt)\n",
    "    text = []\n",
    "    imgMask = loading_color_mask\n",
    "    __stopwords=STOPWORDS.copy()\n",
    "    __stopwords.update([df.loc[0,'from_query_name']])\n",
    "    if sentiment:\n",
    "        text = df[df['sa_score']>0].content.to_list()\n",
    "        imgMask = like_color_mask\n",
    "    else:\n",
    "        text = df[df['sa_score']<0].content.to_list()\n",
    "        imgMask = dislike_color_mask\n",
    "    text = ' '.join(text)\n",
    "    wordcloud = WordCloud(stopwords=__stopwords, background_color='white', mask=imgMask, max_words=200).generate(text)\n",
    "    plt.axis(False)\n",
    "\n",
    "    __tmpfile = BytesIO()\n",
    "    wordcloud.to_image().save(__tmpfile, format='png')\n",
    "    __encoded = base64.b64encode(__tmpfile.getvalue()).decode('utf-8')\n",
    "    imghtml = '<img class=\"wordcloud\" src=\\'data:image/png;base64,{}\\'>'.format(__encoded)\n",
    "    plt.close()\n",
    "    return imghtml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grapKeyword(df, sentiment):\n",
    "    text = []\n",
    "    text = df[df['sa_score']>0].content.to_list() if sentiment else df[df['sa_score']<0].content.to_list()\n",
    "    text = ' '.join(text)\n",
    "    text = remove_actags(rmEmoji(rmURL(text)))\n",
    "    keywords = kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 3), stop_words='english', highlight=False, top_n=25)\n",
    "    keywords_list= list(dict(keywords).keys())\n",
    "    return str(keywords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sony = snfacebook.FacebookUserScraper('https://www.facebook.com/SonyElectronics/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "facebook-user\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
